# -*- coding: utf-8 -*-
"""“01_transformers_multiclass_classification.ipynb”的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OrCb5KA0l1-S1qhPt_PcS2uPtJig6lf_

# Fine Tuning Transformer for MultiClass Text Classification
"""

# from google.colab import drive
# drive.mount('/content/drive')

# Importing the libraries needed
#!pip install transformers datasets
import datasets
import pandas as pd
import numpy as np
import torch
# import transformers
from torch.utils.data import DataLoader
from transformers import DistilBertTokenizerFast
from transformers import DistilBertForSequenceClassification
from transformers import DistilBertModel,DistilBertConfig
from sklearn.metrics import f1_score
# from pprint import pprint
from tqdm import tqdm

BATCH_SIZE=8

def tokenize_fn(X):
    return tok(X["X"], truncation = True, padding="max_length")

def eval_model(model, dataset_val, batch_size=BATCH_SIZE):
    model.train(False)
    f1 = 0


    # create a pytorch data loaders to make it easy to iterate over batches of validation data
    dataloader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)

    y_true = []
    y_pred = []
    with torch.no_grad():
        for batch in tqdm(dataloader):
            y_true.append(batch["Y"].numpy())

            # run the transformer forwards
            outputs = model(
                input_ids = batch["input_ids"].to(device),
                attention_mask = batch["attention_mask"].to(device),
            )
            logits = torch.argmax(outputs.logits,dim=1)
            y_pred.append(logits.cpu().numpy())

    f1 = f1_score(np.concatenate(y_true), np.concatenate(y_pred),average='macro')


    model.train(True)
    return f1


def finetune_model(model, dataset_train, dataset_val=None, eval_model=None, batch_size=BATCH_SIZE, n_epochs=2, learning_rate=1e-5,fmodel='best_model2.pth'):
    model.train(True)

    # create a pytorch data loaders to make it easy to iterate over batches of training data
    # see https://pytorch.org/docs/stable/data.html
    dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)

    n_batches = (len(dataset_train) - 1) // batch_size + 1

    # get the AdamW optimizer
    optimiser = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)
    best_f1=0

    # run the training loop
    # print(f'{"Epoch":>10} {"Batch":>20} {"Loss":>10}')
    print('Training:')
    for epoch in range(n_epochs):
        for (b, batch) in enumerate(tqdm(dataloader)):
            # run the transformer forwards
            outputs = model(
                labels=batch["Y"].to(device),
                input_ids = batch["input_ids"].to(device),
                attention_mask = batch["attention_mask"].to(device),
            )

            # get the classification loss
            loss = outputs.loss

            # backpropagate then apply the optimiser
            optimiser.zero_grad()
            loss.backward()
            optimiser.step()
            # print the loss
            if (b+1) % 25 == 0:
                print(f'{epoch+1:>10} {f"{b+1} / {n_batches}":>20} {format(loss.cpu().item(), ".3f"):>10}')

        # evaluate the model on validation data
        if (dataset_val is not None) and (eval_model is not None):
            print('Evaluating ...')
            f1 = eval_model(model, dataset_val)
            print(f'Epoch {epoch+1}: F1 score (validation) = {format(f1, ".3f")}')
        if f1>best_f1:
            best_f1=f1
            torch.save(model, fmodel)

    return model

def predict(model,test_data,batch_size=BATCH_SIZE):
    model.train(False)
    f1 = 0
    dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)
    y_pred = []
    with torch.no_grad():
        for batch in tqdm(dataloader):
            outputs = model(
                input_ids = batch["input_ids"].to(device),
                attention_mask = batch["attention_mask"].to(device),
            )
            logits = torch.argmax(outputs.logits,dim=1)
            ypre=logits.cpu().numpy()
            y_pred.extend(ypre)
    model.train(True)
    return y_pred
def balance_sample(df,frac,seed):
    return df.groupby('Y', group_keys=False).apply(lambda x: x.sample(frac=frac,random_state=seed))

if __name__=='__main__':
    device=torch.device('mps')
    df_tr = pd.read_json('/Users/yinhao/Desktop/course/COMP6490/A3_code_data/data/genre_train.json' )
    df_tt=pd.read_json('/Users/yinhao/Desktop/course/COMP6490/A3_code_data/data/genre_test.json' )
    frac = 6/7
    train_dataset=balance_sample(df_tr,frac,42)
    val_dataset=df_tr.drop(train_dataset.index).reset_index(drop=True)
    train_dataset= datasets.Dataset.from_pandas(train_dataset)
    val_dataset= datasets.Dataset.from_pandas(val_dataset)
    test_data=datasets.Dataset.from_pandas(df_tt)

    model_config = DistilBertConfig(
        num_labels=4,
        vocab_size=30000,
        max_position_embeddings=512,
        hidden_size=512,
        num_hidden_layers=2,
        num_attention_heads=4,
        # dropout_prob=0.3,
        attention_dropout=0.5

    )
    model = DistilBertForSequenceClassification(config=model_config)
    tok = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')
    shuffle_train_data = train_dataset.shuffle(seed=42)
    shuffle_val_data = val_dataset.shuffle(seed=42)
    tokenized_train_data = shuffle_train_data.map(tokenize_fn, batched=True)
    train_data =tokenized_train_data .with_format("torch", columns=["Y", "input_ids", "attention_mask"])
    tokenized_val_data = shuffle_val_data.map(tokenize_fn, batched=True)
    val_data =tokenized_val_data .with_format("torch", columns=["Y", "input_ids", "attention_mask"])
    test_token=test_data.map(tokenize_fn, batched=True)
    test_data=test_token.with_format("torch", columns=["input_ids", "attention_mask"])
    # model = torch.load('best_model.pth')
    model.to(device)
    model = finetune_model(model, train_data, val_data, eval_model=eval_model, n_epochs=30)
    model.eval()
    Y_test_pred = predict(model,test_data)
    fout = open("out.csv", "w")
    fout.write("Id,Predicted\n")
    for i, line in enumerate(Y_test_pred): # Y_test_pred is in the same order as the test data
        fout.write("%d,%d\n" % (i, int(line)))
    fout.close()

import os
import sys
import pickle
from query import (
    get_doc_to_norm,
    run_query,
)
from indexer import (
    make_doc_ids,
    get_token_list,
    index_from_tokens,
)
from ctransformers import AutoModelForCausalLM


def build_index():
    """build index for the documents."""
    print('Build index ...')
    # get a list of documents
    doc_path = os.path.join("data", "gov", "documents")
    doc_list = [os.path.join(dpath, f) for (dpath, dnames, fnames) in os.walk(doc_path) for f in fnames \
        if not f.startswith('.')]  # excludes UNIX hidden files such as ".DS_Store" generated by MacOS
    num_docs = len(doc_list)
    print(f"Found {num_docs} documents.")

    # assign unique doc_ids to each of the documents
    doc_ids = make_doc_ids(doc_list)
    # get the list of tokens in all the documents
    tok_list = get_token_list(doc_list, doc_ids)

    # build the index from the list of tokens
    index, doc_freq = index_from_tokens(tok_list)
    del tok_list # free some memory

    # compute doc norms (in practice we would want to store this on disk, for
    # simplicity in this assignment it is computed here)
    doc_norms = get_doc_to_norm(index, doc_freq, num_docs) 

    # get a reverse mapping from doc_ids to document paths
    id_to_doc = {docid: path for (path, docid) in doc_ids.items()}

    # store the index to disk
    pickle.dump((index, doc_freq, doc_norms, id_to_doc, num_docs), open("index_data.pkl", "wb"))


def get_top_docs(query, topk=3):
    """query and return the k top ranked documents.

    Args:
        query (str): the query string
        topk (int): the number of top ranked documents to return

    Returns:
        list(str): the list of k top ranked document file names
    """
    # load the index from disk
    index_file = "index_data.pkl"
    if not os.path.exists(index_file):
        build_index()
    (index, doc_freq, doc_norms, id_to_doc, num_docs) = pickle.load(open(index_file, "rb"))
    res = run_query(query, index, doc_freq, doc_norms, num_docs)
    print(f'\nQuery: {query}')
    print(f'Top-{topk} documents (similarity scores):')
    file_paths = []
    for (docid, sim) in res[:topk]:
        fpath = id_to_doc[docid]
        file_paths.append(fpath)
        print(f'{fpath} {sim:.4f}')
    return file_paths


def load_llm():
    """load a pre-trained LLM"""
    model_repo = "TheBloke/Llama-2-7b-Chat-GGUF"  # name of a Hugging Face Hub model repo
    model_file = "llama-2-7b-chat.Q4_K_M.gguf"   # name of the model file in repo
    print(f'loading model "{model_repo}/{model_file}" ...')
    # to work offline, you may set `local_files_only` to True once the model files have been downloaded.
    llm = AutoModelForCausalLM.from_pretrained(model_repo, model_file=model_file, local_files_only=False)
    return llm


def query_llm(llm, prompt, max_new_tokens=256, verbose=True, seed=101, **kwargs):
    """use a pre-trained LLM to generate (and stream) text w.r.t. the given prompt.

    Args:
        llm (LLM): a pre-trained LLM
        prompt (str): the prompt for text generation
        max_new_tokens (int): the maximum number of new tokens may be generated by the LLM
        verbose (boolean): print additional details if True
        seed (int): the seed value to use for sampling tokens

    Returns:
        str: the generated (and streamed) text
    """
    assert len(prompt.strip()) > 0
    kwargs.update({
        'max_new_tokens': max_new_tokens,
        'seed': seed,
        'stream': True,  # whether to stream the generated text
        'reset': True,  # whether to reset the model state before generating text
    })
    
    if verbose:
        sys.stdout.write(f'\n{prompt}')

    ans = []
    for text in llm(prompt, **kwargs):
        print(text, end="", flush=True)
        ans.append(text)
    sys.stdout.write('\n')
    return "".join(ans)


def get_prompt(query, context=None, num_word=50):
    """create a prompt for the given query.

    Args:
        query (str): the query string
        context (str): optional, the context string
        num_word (int): the (approximate) number of words in the generated text

    Returns:
        str: the prompt string
    """
    if context is None:
        return f'Answer the question below in text using about {num_word} words, your answer should be in bullet points.\n\nQuestion:\n{query}\n\nAnswer:\n'
    
    # TODO: create a prompt for the given query and context
    prompt = f'Answer the question below using about {num_word} words. Your answer should be in bullet points.\n\nContext:\n{context}\n\nQuestion:\n{query}\n\nAnswer:\n'
    

    return prompt


def summarise_text(llm, text, num_word=50, seed=101):
    """summarise text using the LLM.

    Args:
        llm (LLM): a pre-trained LLM
        text (str): the text to be summarised
        num_word (int): the (approximate) number of words in the generated summary
        seed (int): the seed value to use for sampling tokens in the LLM

    Returns:
        str: the summary
    """

    # TODO: summarise the given text using a pre-trained LLM
    # you may need to truncate the text so that the prompt and generated summary fit in the context window
    # of the LLM (otherwise it may cause an error "Number of tokens exceeded maximum context length")

    # you may use a different prompt for the summarisation task 
    prompt = lambda x: f"Summarise the below text using about {num_word} words.\n---\n{x}\n---\nSummary:\n"
    summary = ""
    print(f'summary genrating...')
    for t in llm(prompt(text),seed=seed,stream=True):
        print(t,end='',flush=True)
        summary+=t
    return summary


def retrieval_augmented_generation(llm, query, num_top_doc=3, num_word=50, verbose=True, seed=101):
    """generate text using an LLM through Retrieval-Augmented Generation (RAG) for the query.

    Args:
        llm (LLM): a pre-trained LLM
        query (str): the query string
        num_top_doc (int): the number of top ranked documents to be used in RAG
        num_word (int): the (approximate) number of words in the generated text
        verbose (boolean): print additional details if True
        seed (int): the seed value to use for sampling tokens in the LLM

    Returns:
        str: the generated text
    """

    # TODO: implement this function, it should
    # 1) retrieve the top `num_top_doc` ranked documents for `query`
    #    using the `get_top_docs` function;
    # 2) summarise each of the top ranked documents using the `summarise_text` function,
    #    and create a context string e.g. by concatenating the summaries;
    # 3) create an prompt that incorporates the query and the context;
    # 4) generate text using a pre-trained LLM and the prompt, return the generated text.
    # assert len(prompt.strip()) > 0
    # kwargs.update({
    #     'seed': seed,
    #     'stream': True,  # whether to stream the generated text
    #     'reset': True,  # whether to reset the model state before generating text
    # })
    if verbose:
        sys.stdout.write(f'\n{get_prompt}')
    ans = ""
    context=''
    docs=get_top_docs(query,topk=num_top_doc)
    for doc in docs:
      text=open(doc)
      context+=summarise_text(llm,text,num_word=num_word,seed=seed)
    prompt=get_prompt(query,context,num_word=num_word)
    for text in llm(prompt):
        print(text, end="", flush=True)
        ans+=text
    sys.stdout.write('\n')
    return ans


if __name__ == '__main__':
    llm = load_llm()
    for query in [
        'Is nuclear power plant eco-friendly?',
        'How to stay safe during severe weather?',
    ]: 
        # uncomment to directly query the pre-trained LLM
        query_llm(llm, get_prompt(query), verbose=True, seed=101)

        # uncomment to query the pre-trained LLM through RAG
        retrieval_augmented_generation(llm, query, num_top_doc=3, num_word=50, verbose=True, seed=101)


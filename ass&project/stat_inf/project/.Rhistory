#   means<- matrix(params[10:15],3,2)
#   sigma<- array(params[16:27],dim=c(2,2,3))
#   y1<-rbind(data1[,1], data2[,1], data3[,1])
#   y2<-rbind(data1[,1], data2[,1], data3[,1])
#   log_loss<-0
#   for(i in 1:n){
#     x<-t(rbind(y1[,i]%*%solve(lincom), y2[,i]%*%solve(lincom)))
#     log_loss<-log_loss+logpdf_gaussian_mixture(x,means,sigma)
#   }
#   -log_loss
# }
eg_loss <- function(params, data1,data2,data3) {
q=0.01
n<-nrow(data1)
lincom<- matrix(params[1:9],3,3)
means<- matrix(params[10:15],3,2)
sigma<- array(params[16:27],dim=c(2,2,3))
weights<- params[28:30]
y1<-rbind(data1[,1], data2[,1], data3[,1])
y2<-rbind(data1[,1], data2[,1], data3[,1])
p1<-eigen(cor(t(y1)))$vectors
p2<-eigen(cor(t(y2)))$vectors
dis<-NULL
for(i in 1:3){
dis<-rbind(dis,p1[,i]%*%(y1-rep(lincom%*%means[,1],n))+p2[,i]%*%(y2-rep(lincom%*%means[,2],n)))
}
loss<-0
for(i in 1:n){
# x<-t(rbind(y1[,i]%*%solve(lincom), y2[,i]%*%solve(lincom)))
loss<-loss+sum(dis[,i]^2)+q*sum(means%*%t(means))
}
(loss/n)
}
# Initialize parameters
linc <- diag(3)
means <- data[sample(1:nrow(data), k), ] # Randomly initialize means
sigma <- array(0, dim=c(d, d, k))
# for (i in 1:k) {
#   sigma[,,i] <- matrix(c(1,0.5,0.5,1),2,2)
# }
sigma[,,1] <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
sigma[,,2] <- matrix(c(1, -0.7, -0.7, 1), 2, 2)
sigma[,,3]<- matrix(c(1, 0.2, 0.2, 1), 2, 2)
weights <- rep(1/3, 3) # Equal weights initially
# Flatten parameters for optim
params <- c(as.vector(linc),as.vector(means), as.vector(sigma), weights)
# Optimize parameters using optim
opt_result <- optim(par=params, fn=eg_loss,data1=data1,data2=data2,data3=data3, method="L-BFGS-B")
# Extract optimized parameters
opt_params <- opt_result$par
opt_linc<-matrix(opt_params[1:9],3,3)
opt_means <- matrix(opt_params[10:15], 3, 2)
opt_sigma <- array(opt_params[16:27], dim=c(2, 2, 3))
opt_weights <- opt_params[28:30]
# # Plot results
# plot(data, col='blue', pch=19, main="EM with Custom Loss Function")
# colors <- c("red", "green", "blue")
# points(opt_means, col=colors, pch=4, cex=2, lwd=2)
# legend("topright", legend=c("Cluster 1 Mean", "Cluster 2 Mean", "Cluster 3 Mean"),
#        col=colors, pch=4, pt.cex=2, lwd=2)
opt_means
# Load necessary libraries
if(!require(mclust)) install.packages("mclust")
if(!require(MASS)) install.packages("MASS")
library(mclust)
library(MASS)
# Generate synthetic 2D data
set.seed(123)
n <- 1000
mu1 <- c(2, 2)
mu2 <- c(-2, -2)
mu3 <- c(5, -3)
sigma1 <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
sigma2 <- matrix(c(1, -0.7, -0.7, 1), 2, 2)
sigma3 <- matrix(c(1, 0.2, 0.2, 1), 2, 2)
linear_comb<- diag(3)
data1 <- mvrnorm(n/3, mu1, sigma1)
data2 <- mvrnorm(n/3, mu2, sigma2)
data3 <- mvrnorm(n/3, mu3, sigma3)
data<- rbind(data1,data2,data3)
x1 <- t(linear_comb%*%rbind(data1[,1], data2[,1], data3[,1]))
x2 <- t(linear_comb%*%rbind(data1[,2], data2[,2], data3[,2]))
col<-c(rep("green",n/3),rep("red",n/3),rep("blue",n/3))
datax<-cbind(c(x1[,1],x1[,2],x1[,3]),c(x2[,1],x2[,2],x2[,3]))
# Plot the data
plot(datax, col=col, pch=19, main="Generated 2D Data")
# Custom loss function
pdf_gaussian <- function(x, mu, sigma) {
det_sigma <- det(sigma)
const <- 1 / (sqrt((2*pi)^d * det_sigma))
exp_val <- exp(-0.5 * t(x - mu) %*% solve(sigma) %*% (x - mu))
return(const * exp_val)
}
# Define the PDF of the mixture of Gaussians
logpdf_gaussian_mixture <- function(x, mus, sigmas) {
q=1
n <- nrow(mus)
pdf <- 0
for (i in 1:n) {
pdf <- pdf+log( pdf_gaussian(x[i,], mus[i,], sigmas[,,i]))
}
return(pdf)
}
# mle<-function(params, data1,data2,data3){
#    n<-nrow(data1)
#   lincom<- matrix(params[1:9],3,3)
#   means<- matrix(params[10:15],3,2)
#   sigma<- array(params[16:27],dim=c(2,2,3))
#   y1<-rbind(data1[,1], data2[,1], data3[,1])
#   y2<-rbind(data1[,1], data2[,1], data3[,1])
#   log_loss<-0
#   for(i in 1:n){
#     x<-t(rbind(y1[,i]%*%solve(lincom), y2[,i]%*%solve(lincom)))
#     log_loss<-log_loss+logpdf_gaussian_mixture(x,means,sigma)
#   }
#   -log_loss
# }
eg_loss <- function(params, data1,data2,data3) {
q=-1
n<-nrow(data1)
lincom<- matrix(params[1:9],3,3)
means<- matrix(params[10:15],3,2)
sigma<- array(params[16:27],dim=c(2,2,3))
weights<- params[28:30]
y1<-rbind(data1[,1], data2[,1], data3[,1])
y2<-rbind(data1[,1], data2[,1], data3[,1])
p1<-eigen(cor(t(y1)))$vectors
p2<-eigen(cor(t(y2)))$vectors
dis<-NULL
for(i in 1:3){
dis<-rbind(dis,p1[,i]%*%(y1-rep(lincom%*%means[,1],n))+p2[,i]%*%(y2-rep(lincom%*%means[,2],n)))
}
loss<-0
for(i in 1:n){
# x<-t(rbind(y1[,i]%*%solve(lincom), y2[,i]%*%solve(lincom)))
loss<-loss+sum(dis[,i]^2)+q*sum(means%*%t(means))
}
(loss/n)
}
# Initialize parameters
linc <- diag(3)
means <- data[sample(1:nrow(data), k), ] # Randomly initialize means
sigma <- array(0, dim=c(d, d, k))
# for (i in 1:k) {
#   sigma[,,i] <- matrix(c(1,0.5,0.5,1),2,2)
# }
sigma[,,1] <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
sigma[,,2] <- matrix(c(1, -0.7, -0.7, 1), 2, 2)
sigma[,,3]<- matrix(c(1, 0.2, 0.2, 1), 2, 2)
weights <- rep(1/3, 3) # Equal weights initially
# Flatten parameters for optim
params <- c(as.vector(linc),as.vector(means), as.vector(sigma), weights)
# Optimize parameters using optim
opt_result <- optim(par=params, fn=eg_loss,data1=data1,data2=data2,data3=data3, method="L-BFGS-B")
# Extract optimized parameters
opt_params <- opt_result$par
opt_linc<-matrix(opt_params[1:9],3,3)
opt_means <- matrix(opt_params[10:15], 3, 2)
opt_sigma <- array(opt_params[16:27], dim=c(2, 2, 3))
opt_weights <- opt_params[28:30]
# # Plot results
# plot(data, col='blue', pch=19, main="EM with Custom Loss Function")
# colors <- c("red", "green", "blue")
# points(opt_means, col=colors, pch=4, cex=2, lwd=2)
# legend("topright", legend=c("Cluster 1 Mean", "Cluster 2 Mean", "Cluster 3 Mean"),
#        col=colors, pch=4, pt.cex=2, lwd=2)
opt_means
# Load necessary libraries
if(!require(mclust)) install.packages("mclust")
if(!require(MASS)) install.packages("MASS")
library(mclust)
library(MASS)
# Generate synthetic 2D data
set.seed(123)
n <- 1000
mu1 <- c(2, 2)
mu2 <- c(-2, -2)
mu3 <- c(5, -3)
sigma1 <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
sigma2 <- matrix(c(1, -0.7, -0.7, 1), 2, 2)
sigma3 <- matrix(c(1, 0.2, 0.2, 1), 2, 2)
linear_comb<- diag(3)
data1 <- mvrnorm(n/3, mu1, sigma1)
data2 <- mvrnorm(n/3, mu2, sigma2)
data3 <- mvrnorm(n/3, mu3, sigma3)
data<- rbind(data1,data2,data3)
x1 <- t(linear_comb%*%rbind(data1[,1], data2[,1], data3[,1]))
x2 <- t(linear_comb%*%rbind(data1[,2], data2[,2], data3[,2]))
col<-c(rep("green",n/3),rep("red",n/3),rep("blue",n/3))
datax<-cbind(c(x1[,1],x1[,2],x1[,3]),c(x2[,1],x2[,2],x2[,3]))
# Plot the data
plot(datax, col=col, pch=19, main="Generated 2D Data")
# Custom loss function
pdf_gaussian <- function(x, mu, sigma) {
det_sigma <- det(sigma)
const <- 1 / (sqrt((2*pi)^d * det_sigma))
exp_val <- exp(-0.5 * t(x - mu) %*% solve(sigma) %*% (x - mu))
return(const * exp_val)
}
# Define the PDF of the mixture of Gaussians
logpdf_gaussian_mixture <- function(x, mus, sigmas) {
q=1
n <- nrow(mus)
pdf <- 0
for (i in 1:n) {
pdf <- pdf+log( pdf_gaussian(x[i,], mus[i,], sigmas[,,i]))
}
return(pdf)
}
# mle<-function(params, data1,data2,data3){
#    n<-nrow(data1)
#   lincom<- matrix(params[1:9],3,3)
#   means<- matrix(params[10:15],3,2)
#   sigma<- array(params[16:27],dim=c(2,2,3))
#   y1<-rbind(data1[,1], data2[,1], data3[,1])
#   y2<-rbind(data1[,1], data2[,1], data3[,1])
#   log_loss<-0
#   for(i in 1:n){
#     x<-t(rbind(y1[,i]%*%solve(lincom), y2[,i]%*%solve(lincom)))
#     log_loss<-log_loss+logpdf_gaussian_mixture(x,means,sigma)
#   }
#   -log_loss
# }
eg_loss <- function(params, data1,data2,data3) {
q=1
n<-nrow(data1)
lincom<- matrix(params[1:9],3,3)
means<- matrix(params[10:15],3,2)
sigma<- array(params[16:27],dim=c(2,2,3))
weights<- params[28:30]
y1<-rbind(data1[,1], data2[,1], data3[,1])
y2<-rbind(data1[,1], data2[,1], data3[,1])
p1<-eigen(cor(t(y1)))$vectors
p2<-eigen(cor(t(y2)))$vectors
dis<-NULL
for(i in 1:3){
dis<-rbind(dis,p1[,i]%*%(y1-rep(lincom%*%means[,1],n))+p2[,i]%*%(y2-rep(lincom%*%means[,2],n)))
}
loss<-0
for(i in 1:n){
# x<-t(rbind(y1[,i]%*%solve(lincom), y2[,i]%*%solve(lincom)))
loss<-loss+sum(dis[,i]^2)
}
(loss/n)+q*sum(means%*%t(means))
}
# Initialize parameters
linc <- diag(3)
means <- data[sample(1:nrow(data), k), ] # Randomly initialize means
sigma <- array(0, dim=c(d, d, k))
# for (i in 1:k) {
#   sigma[,,i] <- matrix(c(1,0.5,0.5,1),2,2)
# }
sigma[,,1] <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
sigma[,,2] <- matrix(c(1, -0.7, -0.7, 1), 2, 2)
sigma[,,3]<- matrix(c(1, 0.2, 0.2, 1), 2, 2)
weights <- rep(1/3, 3) # Equal weights initially
# Flatten parameters for optim
params <- c(as.vector(linc),as.vector(means), as.vector(sigma), weights)
# Optimize parameters using optim
opt_result <- optim(par=params, fn=eg_loss,data1=data1,data2=data2,data3=data3, method="L-BFGS-B")
# Extract optimized parameters
opt_params <- opt_result$par
opt_linc<-matrix(opt_params[1:9],3,3)
opt_means <- matrix(opt_params[10:15], 3, 2)
opt_sigma <- array(opt_params[16:27], dim=c(2, 2, 3))
opt_weights <- opt_params[28:30]
# # Plot results
# plot(data, col='blue', pch=19, main="EM with Custom Loss Function")
# colors <- c("red", "green", "blue")
# points(opt_means, col=colors, pch=4, cex=2, lwd=2)
# legend("topright", legend=c("Cluster 1 Mean", "Cluster 2 Mean", "Cluster 3 Mean"),
#        col=colors, pch=4, pt.cex=2, lwd=2)
q*sum(means%*%t(means))
opt_means
library(xgboost) # for xgboost
library(tidyverse)
library(ggplot2)
# Load the data
data <- read.csv("kerala.csv")
head(data)
# Select numeric columns and exclude the 'YEAR' column
data_x <- data %>%
select_if(is.numeric) %>%
select(-YEAR)
library(xgboost) # for xgboost
library(tidyverse)
library(ggplot2)
# Load the data
data <- read.csv("kerala.csv")
head(data)
# Select numeric columns and exclude the 'YEAR' column
data_x <- data %>%
select_if(is.numeric) %>%
select(-YEAR)
library(xgboost) # for xgboost
library(tidyverse)
library(ggplot2)
# Load the data
data <- read.csv("kerala.csv")
head(data)
# Select numeric columns and exclude the 'YEAR' column
data_x <- data %>%
select_if(is.numeric) %>%
select(-YEAR)
library(xgboost) # for xgboost
library(tidyverse)
library(ggplot2)
# Load the data
data <- read.csv("kerala.csv")
head(data)
# Select numeric columns and exclude the 'YEAR' column
data_x <- data %>%
select_if(is.numeric) %>%
select(-YEAR)
library(xgboost) # for xgboost
library(tidyverse)
library(ggplot2)
# Load the data
data <- read.csv("kerala.csv")
head(data)
# Select numeric columns and exclude the 'YEAR' column
data_x <- data %>%
select_if(is.numeric) %>%
select(-YEAR)
# Select numeric columns and exclude the 'YEAR' column
data_x <- data %>%
select_if(is.numeric) %>%select(-YEAR)
# Select numeric columns and exclude the 'YEAR' column
data_x <- data %>%
select_if(is.numeric)
head(data_x)
# Load the data
data <- read.csv("kerala.csv")
head(data)
# Select numeric columns and exclude the 'YEAR' column
data_x <- data %>%
select_if(is.numeric)
# Select numeric columns and exclude the 'YEAR' column
data_x <- data %>%
select_if(is.numeric) %>%
select(YEAR)
# Select numeric columns and exclude the 'YEAR' column
data_x <- data %>%
select_if(is.numeric) %>%
select(-'YEAR')
# Load necessary libraries
if(!require(mclust)) install.packages("mclust")
if(!require(MASS)) install.packages("MASS")
library(mclust)
library(MASS)
# Generate synthetic 2D data
set.seed(123)
n <- 1000
mu1 <- c(2, 2)
mu2 <- c(-2, -2)
mu3 <- c(5, -3)
sigma1 <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
sigma2 <- matrix(c(1, -0.7, -0.7, 1), 2, 2)
sigma3 <- matrix(c(1, 0.2, 0.2, 1), 2, 2)
linear_comb<- diag(3)
data1 <- mvrnorm(n/3, mu1, sigma1)
data2 <- mvrnorm(n/3, mu2, sigma2)
data3 <- mvrnorm(n/3, mu3, sigma3)
data<- rbind(data1,data2,data3)
x1 <- t(linear_comb%*%rbind(data1[,1], data2[,1], data3[,1]))
x2 <- t(linear_comb%*%rbind(data1[,2], data2[,2], data3[,2]))
col<-c(rep("green",n/3),rep("red",n/3),rep("blue",n/3))
datax<-cbind(c(x1[,1],x1[,2],x1[,3]),c(x2[,1],x2[,2],x2[,3]))
# Plot the data
plot(datax, col=col, pch=19, main="Generated 2D Data")
# Custom loss function
pdf_gaussian <- function(x, mu, sigma) {
det_sigma <- det(sigma)
const <- 1 / (sqrt((2*pi)^d * det_sigma))
exp_val <- exp(-0.5 * t(x - mu) %*% solve(sigma) %*% (x - mu))
return(const * exp_val)
}
# Define the PDF of the mixture of Gaussians
logpdf_gaussian_mixture <- function(x, mus, sigmas) {
q=1
n <- nrow(mus)
pdf <- 0
for (i in 1:n) {
pdf <- pdf+log( pdf_gaussian(x[i,], mus[i,], sigmas[,,i]))
}
return(pdf)
}
# mle<-function(params, data1,data2,data3){
#    n<-nrow(data1)
#   lincom<- matrix(params[1:9],3,3)
#   means<- matrix(params[10:15],3,2)
#   sigma<- array(params[16:27],dim=c(2,2,3))
#   y1<-rbind(data1[,1], data2[,1], data3[,1])
#   y2<-rbind(data1[,1], data2[,1], data3[,1])
#   log_loss<-0
#   for(i in 1:n){
#     x<-t(rbind(y1[,i]%*%solve(lincom), y2[,i]%*%solve(lincom)))
#     log_loss<-log_loss+logpdf_gaussian_mixture(x,means,sigma)
#   }
#   -log_loss
# }
eg_loss <- function(params, data1,data2,data3) {
q=1
n<-nrow(data1)
lincom<- matrix(params[1:9],3,3)
means<- matrix(params[10:15],3,2)
sigma<- array(params[16:27],dim=c(2,2,3))
weights<- params[28:30]
y1<-rbind(data1[,1], data2[,1], data3[,1])
y2<-rbind(data1[,1], data2[,1], data3[,1])
p1<-eigen(cor(t(y1)))$vectors
p2<-eigen(cor(t(y2)))$vectors
dis<-NULL
for(i in 1:3){
dis<-rbind(dis,p1[,i]%*%(y1-rep(lincom%*%means[,1],n))+p2[,i]%*%(y2-rep(lincom%*%means[,2],n)))
}
loss<-0
for(i in 1:n){
# x<-t(rbind(y1[,i]%*%solve(lincom), y2[,i]%*%solve(lincom)))
loss<-loss+sum(dis[,i]^2)
}
(loss/n)+q*sum(means%*%t(means))
}
# Initialize parameters
linc <- diag(3)
means <- data[sample(1:nrow(data), k), ] # Randomly initialize means
sigma <- array(0, dim=c(d, d, k))
# for (i in 1:k) {
#   sigma[,,i] <- matrix(c(1,0.5,0.5,1),2,2)
# }
sigma[,,1] <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
sigma[,,2] <- matrix(c(1, -0.7, -0.7, 1), 2, 2)
sigma[,,3]<- matrix(c(1, 0.2, 0.2, 1), 2, 2)
weights <- rep(1/3, 3) # Equal weights initially
# Flatten parameters for optim
params <- c(as.vector(linc),as.vector(means), as.vector(sigma), weights)
# Optimize parameters using optim
opt_result <- optim(par=params, fn=eg_loss,data1=data1,data2=data2,data3=data3, method="L-BFGS-B")
# Extract optimized parameters
opt_params <- opt_result$par
opt_linc<-matrix(opt_params[1:9],3,3)
opt_means <- matrix(opt_params[10:15], 3, 2)
opt_sigma <- array(opt_params[16:27], dim=c(2, 2, 3))
opt_weights <- opt_params[28:30]
# # Plot results
# plot(data, col='blue', pch=19, main="EM with Custom Loss Function")
# colors <- c("red", "green", "blue")
# points(opt_means, col=colors, pch=4, cex=2, lwd=2)
# legend("topright", legend=c("Cluster 1 Mean", "Cluster 2 Mean", "Cluster 3 Mean"),
#        col=colors, pch=4, pt.cex=2, lwd=2)
library(xgboost) # for xgboost
library(tidyverse)
library(ggplot2)
# Load the data
data <- read.csv("kerala.csv")
head(data)
# Select numeric columns and exclude the 'YEAR' column
data_x <- data %>%
select_if(is.numeric) %>%
select(- YEAR)
head(data_x)
# Convert the selected data to a numeric matrix
mx <- data.matrix(data_x)
# Convert the 'FLOODS' column to a binary numeric vector
data_y <- ifelse(data$FLOODS == 'YES', 1, 0)
head(data_y)
data_combined <- cbind(data_x, FLOODS = data_y)
# Calculate the correlation matrix
correlation_matrix <- cor(data_combined)
# Plot the correlation matrix
ggcorr(correlation_matrix,
label = TRUE,
label_round = 2,
label_size = 3,
hjust = 0.75,
size = 3,
color = "grey50",
layout.exp = 1,
legend.position = "bottom") +
ggtitle("Correlation Matrix between Features and Target Variable (FLOODS)") +
theme_minimal()
install.packages('IRkernel')
IRkernel::installspec(user = FALSE)  # To register the kernel in the Jupyter
install.packages('IRkernel')
install.packages("IRkernel")
IRkernel::installspec(user = FALSE)
IRkernel::installspec(user = FALSE)
library(jupyter)
library(jupyter)
library(jupyter)
install.packages('jupyter')
install.packages('jupyter_client')
library(jupyter)
library(jupter_client)
library(jupyter_client)
install.packages('devtools')
devtools::install_github('IRkernel/IRkernel')
IRkernel::installspec()
getwd
getwd()
library(xgboost) # for xgboost
library(tidyverse)
library(ggplot2)
# Load the data
data <- read.csv("kerala.csv")
head(data)
# Select numeric columns and exclude the 'YEAR' column
data_x <- data %>%
select_if(is.numeric) %>%
select(-'YEAR')
system("jupyter --version")
IRkernel::installspec(user = FALSE)
